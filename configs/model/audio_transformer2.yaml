name: EXP_TR2

#Seanet encoder config
encoder:
  _target_: modules.models.SEANetEncoder2d
  in_channels: 1
  base_channels: 64
  dimension: 512
  n_residual_layers: 3
  ratios: [8, 5, 4, 2]         
  kernel_size: 7
  last_kernel_size: 7
  residual_kernel_size: 3
  dilation_base: 2
  use_transformer: true
  config_transformer:
    d_model: 512
    n_heads: 4
    n_layers: 4
    d_ff: 2048
    max_seq_len: 2048
    dropout: 0.0
    use_flash: true

#Transformer Decocer config
transformer:
  _target_: modules.transformer_layers.Transformer
  n_heads: 4
  n_layers: 8
  d_ff: 2048
  max_audio_len: 5000
  dropout: 0.0
  conditional: false
  use_flash: true

  # --- Model dimensions ---
  d_model: 512                
  head_dim: 64                # Dimension per attention head: head_dim x n_heads = d_model
  n_heads: 8                  
  num_key_value_heads: 4      
  d_ff: 2048                  
  dropout: 0.1                

  # --- Architecture ---
  num_hidden_layers: 8        # Number of Transformer blocks
  vocab_size: 0           # Size of output vocabulary / token space
  apply_sliding: false         # Whether to apply sliding window to every other layer
  sliding_window: 0         # Attention window length (0 = full)
  use_flash: true            # Placeholder flag (currently not used)
  conditional: false           # If true, expects class embeddings

  # --- Rotary Embedding (RoPE / YaRN / NTK scaling) ---
  rope_theta: 10000           # Base frequency (common: 10000)
  initial_context_length: 1024
  rope_scaling_factor: 1.0    # 1.0 = no scaling, >1.0 enables YaRN/NTK scaling
  rope_ntk_alpha: 1.0         # NTK alpha parameter
  rope_ntk_beta: 32.0         # NTK beta parameter

  # --- RMSNorm ---
  rms_eps: 1.0e-5

  # --- Training and optimization ---
  dtype: bfloat16             # Computation dtype
  device: cuda                # Default device
  max_sequence_length: 1024   # Max tokens in decoder (for training)


